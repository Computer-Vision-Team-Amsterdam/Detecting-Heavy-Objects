"""
This module can be used to upload metadata of unblurred images
as well as predictions of the container detection model.
"""

import argparse
import json
from contextlib import contextmanager
from datetime import datetime
from typing import Dict, List, Optional, Union, Generator, Tuple

import psycopg2
from psycopg2.extras import execute_values

from utils.azure_storage import BaseAzureClient, StorageAzureClient
from utils.date import get_start_date

azClient = BaseAzureClient()
USERNAME = azClient.get_secret_value("postgresUsername")
PASSWORD = azClient.get_secret_value("postgresPassword")
HOST = azClient.get_secret_value("postgresHostname")
PORT = "5432"
DATABASE = "container-detection-database"
from datetime import datetime, timedelta
import pandas as pd
import shutil

import requests
from requests.auth import HTTPBasicAuth
import os
from pathlib import Path

BASE_URL = azClient.get_secret_value("CloudVpsRawUrlTwee")
USERNAMEtwee = azClient.get_secret_value("CloudVpsRawUsername")
PASSWORDtwee = azClient.get_secret_value("CloudVpsRawPassword")

@contextmanager
def connect() -> Generator[Tuple[psycopg2.extensions.connection, psycopg2.extensions.cursor], None, None]:
    """
    Connect to the postgres database.
    """
    try:
        # Connect to an existing database
        conn: psycopg2.extensions.connection = psycopg2.connect(
            user=f"{USERNAME}@{HOST}",
            password=PASSWORD,
            host=f"{HOST}.postgres.database.azure.com",
            port=PORT,
            database=DATABASE,
            keepalives=1,
            keepalives_idle=60,  # Sends a keepalive message after 60 seconds of inactivity
        )
        conn.autocommit = True
        cur: psycopg2.extensions.cursor = conn.cursor()
        yield conn, cur
    except Exception as error:
        raise Exception("Error while connecting to PostgreSQL: " + str(error))
    finally:
        cur.close()
        conn.close()


def get_column_names(table_name: str, cur: psycopg2.extensions.cursor) -> List[str]:
    """
    Get names of the columns in database table.

    :param table_name: name of the table in postgres
    :param cur: cursor to parse the table

    return: list of column names, excluding auto-generated primary key
    """

    sql = f"""SELECT * FROM {table_name}"""
    cur.execute(sql)
    cols = [desc[0] for desc in cursor.description]

    # currently all tables' PK have 'id' in them.
    # For 'images' table, PK=file_name, NOT autogenerated
    if "id" in cols[0]:
        del cols[0]

    return cols


def row_to_upload(
    data_element: Dict[str, Union[str, float]],
    object_fields: List[str],
    table_columns: List[str],
) -> Dict[str, Union[str, float]]:
    """
    Creates row with data to upload to table such that
    the object keys match the table columns.

    :param data_element: data to upload in a single row
    :param object_fields: fields from the data object to be considered for upload
    :param table_columns: corresponding columns from table

    :return: object with ordered row-content to be inserted into table
    """
    row: Dict[str, Union[str, float]] = {key: "" for key in table_columns}

    if len(row) is not len(table_columns):
        raise ValueError("Amount of values does not match amount of columns.")

    for i, column_name in enumerate(table_columns):
        row[column_name] = data_element[object_fields[i]]

    return row


def row_to_upload_from_panorama(
    panorama_id: str, table_columns: List[str]
) -> Dict[str, Union[str, float, datetime]]:
    """
    Creates row with data to upload to table such that the object keys match
    the table columns. Similar to row_to_upload(), but the structure of
    Panorama object makes it difficult to use a for-loop.
    Hence, this is a separate function.

    :param panorama_id: panorama id to query the API with.
    :param table_columns: columns from table corresponding to
                        the selected fields from the panorama object

    :return: object with ordered row-content to be inserted into table
    """
    from panorama.client import PanoramaClient

    pano_object = PanoramaClient.get_panorama(panorama_id)
    row: Dict[str, Union[str, float, datetime]] = {key: "" for key in table_columns}

    row["file_name"] = pano_object.id + ".jpg"
    row["camera_location_lat"] = pano_object.geometry.coordinates[1]
    row["camera_location_lon"] = pano_object.geometry.coordinates[0]
    row["heading"] = pano_object.heading
    row["taken_at"] = pano_object.timestamp

    if len(row) is not len(table_columns):
        raise ValueError("Amount of values does not match amount of columns.")

    return row


def upload_images(cursor_: psycopg2.extensions.cursor, all_rows) -> None:
    keys = get_column_names("images", cursor_)  # column names from table in postgres

    # rows: List[Dict[str, Union[str, float, datetime]]] = [
    #     row_to_upload_from_panorama(element, keys)
    #     for element in data
    # ]
    print(all_rows)

    try:
        query = f"INSERT INTO images ({','.join(keys)}) VALUES %s ON CONFLICT DO NOTHING;"
        execute_values(cursor_, query, all_rows)
    except psycopg2.Error as e:
        # This is the base class for all exceptions raised by psycopg2.
        # It is a catch-all exception that can be raised for any error that
        # occurs during the execution of a query.
        raise e


def upload_detections(cursor_: psycopg2.extensions.cursor, data: List[Dict[str, Union[str, float, datetime]]]) -> None:
    object_fields = ["pano_id", "score", "bbox"]
    keys = get_column_names("detections", cursor_)  # column names from table in postgres

    rows: List[Dict[str, Union[str, float, datetime]]] = [
        row_to_upload(element, object_fields, keys)  # type: ignore
        for element in data
    ]
    values = [list(item.values()) for item in rows]

    try:
        query = f"INSERT INTO detections ({','.join(keys)}) VALUES %s;"
        execute_values(cursor_, query, values)
    except psycopg2.Error as e:
        # This is the base class for all exceptions raised by psycopg2.
        # It is a catch-all exception that can be raised for any error that
        # occurs during the execution of a query.
        raise e

def download_csv_from_cloudvps(
    date: datetime, tja) -> str:
    """
    Downloads panorama from cloudvps to local folder.
    """

    all_rows = []
    for run in tja:
        try:
            url = (
                BASE_URL + f"{date.year}/"
                f"{str(date.month).zfill(2)}/"
                f"{str(date.day).zfill(2)}/"
                f"{run}/panorama1.csv"
            )

            response = requests.get(
                url, timeout=20, stream=True, auth=HTTPBasicAuth(USERNAMEtwee, PASSWORDtwee)
            )
            if response.status_code == 404:
                print(f"No resource found at {url}")

            if response.status_code != 200:
                print(f"Status code is {response.status_code}")

            filename = Path(os.getcwd(), f"{run}.csv")
            with open(filename, "wb") as out_file:
                shutil.copyfileobj(response.raw, out_file)
            del response

            # print(f"{panorama_id} completed.")

        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error: Failed for panorama {run}:\n{e}")
            return ""
        except requests.exceptions.Timeout as e:
            print(f"Timeout Error: Failed for panorama {run}:\n{e}")
            return ""
        except requests.exceptions.ConnectionError as e:
            # In the event of a network problem (e.g. DNS failure, refused connection, etc),
            # Requests will raise a ConnectionError exception.
            print(f"Connection error: Failed for panorama {run}:\n{e}")
            return ""
        except requests.exceptions.RequestException as e:
            print(f"Unknown Error: Failed for panorama {run}:\n{e}")
            return ""

        df = pd.read_csv(filename, sep='\t')
        print(filename)
        print(os.path.isfile(filename))
        print(df)
        df["panorama_file_name"] = df["panorama_file_name"].apply(lambda x: run + "_" + x + ".jpg")
        # Get yesterday's date
        yesterday = datetime.now() - timedelta(days=1)
        yesterday_with_time = datetime.combine(yesterday, datetime.min.time())

        # Add a new column with yesterday's date
        df = df.assign(yesterday_date=yesterday_with_time)
        print(df)
        nested_list = df[["panorama_file_name", "latitude[deg]", "longitude[deg]", "heading[deg]", "yesterday_date"]].values.tolist()

        all_rows.extend(nested_list)

    return all_rows

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--table",
        type=str,
        choices=["images", "detections"],
        help="Table in postgres where to upload data",
    )
    parser.add_argument(
        "--date",
        type=str,
        help="Processing date in the format %Y-%m-%d %H:%M:%S.%f",
    )
    opt = parser.parse_args()

    object_fields_to_select: List[Optional[str]] = []
    saClient = StorageAzureClient(secret_key="data-storage-account-url")

    start_date_dag, start_date_dag_ymd = get_start_date(opt.date)

    if opt.table == "images":
        # Download txt file(s) with pano ids that we want to download from CloudVPS
        cname_input = "retrieve-images-input"
        input_files = saClient.list_container_content(
            cname=cname_input,
            blob_prefix=start_date_dag_ymd,
        )
        print(
            f"Found {len(input_files)} file(s) in container {cname_input} on date {start_date_dag_ymd}."
        )

        # # Download files from CloudVPS
        # input_data_images = []
        # for input_file in input_files:
        #     local_file = input_file.split("/")[1]  # only get file name, without prefix
        #     saClient.download_blob(
        #         cname=cname_input,
        #         blob_name=input_file,
        #         local_file_path=local_file,
        #     )
        #     with open(local_file, "r") as f:
        #         input_data_images.extend([line.rstrip("\n") for line in f])

        tja = ["TMX7316010203-003044", "TMX7316010203-003045", "TMX7316010203-003046", "TMX7316010203-003047"]

        all_rows = download_csv_from_cloudvps(datetime.strptime(start_date_dag_ymd, "%Y-%m-%d"),
                                              tja)

        try:
            with connect() as (_, cursor):
                upload_images(cursor, all_rows)
        except Exception as e:
            raise Exception("Error in upload_images or with connect(): " + str(e))

    if opt.table == "detections":
        # download detections file from the storage account
        input_file_path = "coco_instances_results.json"
        saClient.download_blob(
            cname="detections",
            blob_name=f"{start_date_dag}/coco_instances_results.json",
            local_file_path=input_file_path,
        )

        f = open(input_file_path)
        input_data_detections = json.load(f)

        with connect() as (_, cursor):
            upload_detections(cursor, input_data_detections)
