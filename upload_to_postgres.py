"""
This module can be used to upload metadata of unblurred images
as well as predictions of the container detection model.
"""

import argparse
import json
from contextlib import contextmanager
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Union

import psycopg2
from psycopg2._psycopg import connection  # pylint: disable-msg=E0611
from psycopg2._psycopg import cursor  # pylint: disable-msg=E0611
from psycopg2.errors import ConnectionException  # pylint: disable-msg=E0611
from psycopg2.extras import execute_values

from utils.azure_storage import BaseAzureClient, StorageAzureClient
from utils.date import get_start_date

azClient = BaseAzureClient()
USERNAME = azClient.get_secret_value("postgresUsername")
PASSWORD = azClient.get_secret_value("postgresPassword")
HOST = azClient.get_secret_value("postgresHostname")
PORT = "5432"
DATABASE = "container-detection-database"


@contextmanager
def connect() -> cursor:
    """
    Connect to the postgres database.
    """
    conn = None
    cur = None

    try:
        # Connect to an existing database
        conn = psycopg2.connect(
            user=f"{USERNAME}@{HOST}",
            password=PASSWORD,
            host=f"{HOST}.postgres.database.azure.com",
            port=PORT,
            database=DATABASE,
        )
        conn.autocommit = True
        cur = conn.cursor()
        yield cur
    except ConnectionException as error:
        print("Error while connecting to PostgreSQL", error)
    finally:
        cur.close()
        conn.close()


def get_column_names(table_name: str, cur: cursor) -> List[str]:
    """
    Get names of the columns in database table.

    :param table_name: name of the table in postgres
    :param cur: cursor to parse the table

    return: list of column names, excluding auto-generated primary key
    """

    sql = f"""SELECT * FROM {table_name}"""
    cur.execute(sql)
    cols = [desc[0] for desc in cursor.description]

    # currently all tables' PK have 'id' in them.
    # For 'images' table, PK=file_name, NOT autogenerated
    if "id" in cols[0]:
        del cols[0]

    return cols


def row_to_upload(
    data_element: Dict[str, Union[str, float]],
    object_fields: List[str],
    table_columns: List[str],
) -> Dict[str, Union[str, float]]:
    """
    Creates row with data to upload to table such that
    the object keys match the table columns.

    :param data_element: data to upload in a single row
    :param object_fields: fields from the data object to be considered for upload
    :param table_columns: corresponding columns from table

    :return: object with ordered row-content to be inserted into table
    """
    row: Dict[str, Union[str, float]] = {key: "" for key in table_columns}

    if len(row) is not len(table_columns):
        raise ValueError("Amount of values does not match amount of columns.")

    for i, column_name in enumerate(table_columns):
        row[column_name] = data_element[object_fields[i]]

    return row


def row_to_upload_from_panorama(
    panorama_id: str, table_columns: List[str]
) -> Dict[str, Union[str, float, datetime]]:
    """
    Creates row with data to upload to table such that the object keys match
    the table columns. Similar to row_to_upload(), but the structure of
    Panorama object makes it difficult to use a for-loop.
    Hence, this is a separate function.

    :param panorama_id: panorama id to query the API with.
    :param table_columns: columns from table corresponding to
                        the selected fields from the panorama object

    :return: object with ordered row-content to be inserted into table
    """
    from panorama.client import PanoramaClient

    pano_object = PanoramaClient.get_panorama(panorama_id)
    row: Dict[str, Union[str, float, datetime]] = {key: "" for key in table_columns}

    row["file_name"] = pano_object.id + ".jpg"
    row["camera_location_lat"] = pano_object.geometry.coordinates[1]
    row["camera_location_lon"] = pano_object.geometry.coordinates[0]
    row["heading"] = pano_object.heading
    row["taken_at"] = pano_object.timestamp

    if len(row) is not len(table_columns):
        raise ValueError("Amount of values does not match amount of columns.")

    return row


def upload_images(cursor_, data):
    keys = get_column_names("images", cursor_)  # column names from table in postgres
    query = f"INSERT INTO images ({','.join(keys)}) VALUES %s ON CONFLICT DO NOTHING;"

    rows: List[Dict[str, Union[str, float, datetime]]] = [
        row_to_upload_from_panorama(element, table_columns)  # type: ignore
        for element in data
    ]
    values = [list(item.values()) for item in rows]

    execute_values(cursor_, query, values)


def upload_detections(cursor_, data):
    object_fields = ["pano_id", "score", "bbox"]
    keys = get_column_names("detections", cursor_)  # column names from table in postgres
    query = f"INSERT INTO detections ({','.join(keys)}) VALUES %s;"

    rows: List[Dict[str, Union[str, float, datetime]]] = [
        row_to_upload(element, object_fields, keys)  # type: ignore
        for element in data
    ]
    values = [list(item.values()) for item in rows]

    execute_values(cursor_, query, values)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--table",
        type=str,
        choices=["images", "detections"],
        help="Table in postgres where to upload data",
    )
    parser.add_argument(
        "--date",
        type=str,
        help="Processing date in the format %Y-%m-%d %H:%M:%S.%f",
    )
    opt = parser.parse_args()

    object_fields_to_select: List[Optional[str]] = []
    saClient = StorageAzureClient(secret_key="data-storage-account-url")

    start_date_dag, start_date_dag_ymd = get_start_date(opt.date)

    if opt.table == "images":
        # Download txt file(s) with pano ids that we want to download from CloudVPS
        cname_input = "retrieve-images-input"
        input_files = saClient.list_container_content(
            cname=cname_input,
            blob_prefix=start_date_dag_ymd,
        )
        print(
            f"Found {len(input_files)} file(s) in container {cname_input} on date {start_date_dag_ymd}."
        )

        print(input_files)  # TODO remove

        # Download files from CloudVPS
        input_data = []
        for input_file in input_files:
            local_file = input_file.split("/")[1]  # only get file name, without prefix
            saClient.download_blob(
                cname=cname_input,
                blob_name=input_file,
                local_file_path=local_file,
            )
            with open(local_file, "r") as f:
                input_data = [line.rstrip("\n") for line in f]

        with connect() as cursor:
            upload_images(cursor, input_data)

    if opt.table == "detections":
        # download detections file from the storage account
        input_file_path = "coco_instances_results.json"
        saClient.download_blob(
            cname="detections",
            blob_name=f"{start_date_dag}/coco_instances_results.json",
            local_file_path=input_file_path,
        )

        f = open(input_file_path)
        input_data = json.load(f)

        with connect() as cursor:
            upload_detections(cursor, input_data)
